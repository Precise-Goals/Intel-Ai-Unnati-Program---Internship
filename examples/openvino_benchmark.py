"""
OpenVINO Integration Benchmark Demo

Compares PyTorch vs Intel OpenVINO inference latency for:
1. Text Classification (Sentiment Analysis)
2. Text Embeddings

Requirements:
    pip install optimum[openvino] transformers torch

Usage:
    python examples/openvino_benchmark.py
"""

import sys
import os
import json
from datetime import datetime

sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# Check if OpenVINO is available
try:
    from optimum.intel import OVModelForSequenceClassification
    OPENVINO_AVAILABLE = True
except ImportError:
    OPENVINO_AVAILABLE = False
    print("âš ï¸  OpenVINO not installed. Install with: pip install optimum[openvino]")
    print("    Running in simulation mode for demonstration.\n")


def divider(title: str) -> None:
    print(f"\n{'='*70}")
    print(f"  {title}")
    print('='*70)


# =============================================================================
# Test Data
# =============================================================================

TEST_TEXTS = [
    "This product is absolutely amazing! Best purchase I've ever made.",
    "Terrible quality, broke after one day. Complete waste of money.",
    "The movie was okay, nothing special but not terrible either.",
    "I love how this software makes my work so much easier.",
    "The customer service was unhelpful and rude.",
    "Great value for the price, would recommend to others.",
    "The food was cold and the service was slow.",
    "This book changed my perspective on life completely.",
    "Not impressed with the build quality at all.",
    "Fantastic experience from start to finish!",
]

# =============================================================================
# Main Benchmark
# =============================================================================

if __name__ == "__main__":
    divider("OpenVINO Integration Benchmark")
    print(f"Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"OpenVINO Available: {OPENVINO_AVAILABLE}")
    
    if OPENVINO_AVAILABLE:
        from framework.openvino_tools import (
            OpenVINOTextClassifier,
            OpenVINOEmbedding,
            compare_backends,
            print_benchmark_comparison
        )
        
        # Configuration
        NUM_ITERATIONS = 50  # Reduce for faster demo
        WARMUP_ITERATIONS = 5
        
        # =================================================================
        # Benchmark 1: Text Classification (Sentiment Analysis)
        # =================================================================
        divider("Benchmark 1: Text Classification (Sentiment Analysis)")
        print(f"Model: distilbert-base-uncased-finetuned-sst-2-english")
        print(f"Iterations: {NUM_ITERATIONS} (+ {WARMUP_ITERATIONS} warmup)")
        
        try:
            pytorch_result, openvino_result, comparison = compare_backends(
                model_class=OpenVINOTextClassifier,
                model_name="distilbert-base-uncased-finetuned-sst-2-english",
                test_texts=TEST_TEXTS,
                num_iterations=NUM_ITERATIONS,
                warmup_iterations=WARMUP_ITERATIONS
            )
            
            print_benchmark_comparison(comparison)
            
            # Save results
            results_dir = os.path.join(os.path.dirname(__file__), "benchmark_results")
            os.makedirs(results_dir, exist_ok=True)
            
            results_file = os.path.join(results_dir, f"classification_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(results_file, "w") as f:
                json.dump(comparison, f, indent=2)
            print(f"Results saved to: {results_file}")
            
        except Exception as e:
            print(f"âŒ Classification benchmark failed: {e}")
        
        # =================================================================
        # Benchmark 2: Text Embeddings
        # =================================================================
        divider("Benchmark 2: Text Embeddings")
        print(f"Model: sentence-transformers/all-MiniLM-L6-v2")
        print(f"Iterations: {NUM_ITERATIONS} (+ {WARMUP_ITERATIONS} warmup)")
        
        try:
            pytorch_result, openvino_result, comparison = compare_backends(
                model_class=OpenVINOEmbedding,
                model_name="sentence-transformers/all-MiniLM-L6-v2",
                test_texts=TEST_TEXTS,
                num_iterations=NUM_ITERATIONS,
                warmup_iterations=WARMUP_ITERATIONS
            )
            
            print_benchmark_comparison(comparison)
            
            results_file = os.path.join(results_dir, f"embedding_benchmark_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json")
            with open(results_file, "w") as f:
                json.dump(comparison, f, indent=2)
            print(f"Results saved to: {results_file}")
            
        except Exception as e:
            print(f"âŒ Embedding benchmark failed: {e}")
        
        # =================================================================
        # Demo: Using OpenVINO Classifier
        # =================================================================
        divider("Demo: OpenVINO Text Classification")
        
        classifier = OpenVINOTextClassifier(use_openvino=True)
        classifier.load()
        
        print("\nSample Classifications:")
        for text in TEST_TEXTS[:5]:
            result = classifier.classify(text)
            sentiment = "ğŸ˜Š POSITIVE" if result['label'] == 'POSITIVE' else "ğŸ˜ NEGATIVE"
            print(f"  {sentiment} ({result['confidence']:.1%}): {text[:50]}...")
        
    else:
        # Simulation mode when OpenVINO is not installed
        divider("Simulated Benchmark Results (OpenVINO not installed)")
        
        print("""
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    SIMULATED BENCHMARK RESULTS                      â”‚
â”‚           (Install OpenVINO to run actual benchmarks)               â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  Model: distilbert-base-uncased-finetuned-sst-2-english            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metric                  â”‚ PyTorch       â”‚ OpenVINO      â”‚ Improve   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Avg Latency (ms)        â”‚        45.23  â”‚        28.41  â”‚    37.2%  â”‚
â”‚ Min Latency (ms)        â”‚        42.18  â”‚        26.54  â”‚           â”‚
â”‚ Max Latency (ms)        â”‚        51.87  â”‚        32.19  â”‚           â”‚
â”‚ P50 Latency (ms)        â”‚        44.92  â”‚        28.03  â”‚           â”‚
â”‚ P95 Latency (ms)        â”‚        49.31  â”‚        30.87  â”‚           â”‚
â”‚ Throughput (req/sec)    â”‚        22.11  â”‚        35.21  â”‚    59.2%  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SPEEDUP FACTOR:                                             1.59x   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Model: sentence-transformers/all-MiniLM-L6-v2                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Metric                  â”‚ PyTorch       â”‚ OpenVINO      â”‚ Improve   â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ Avg Latency (ms)        â”‚        12.87  â”‚         7.94  â”‚    38.3%  â”‚
â”‚ Min Latency (ms)        â”‚        11.92  â”‚         7.21  â”‚           â”‚
â”‚ Max Latency (ms)        â”‚        15.43  â”‚         9.18  â”‚           â”‚
â”‚ P50 Latency (ms)        â”‚        12.65  â”‚         7.82  â”‚           â”‚
â”‚ P95 Latency (ms)        â”‚        14.21  â”‚         8.76  â”‚           â”‚
â”‚ Throughput (req/sec)    â”‚        77.73  â”‚       125.94  â”‚    62.0%  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ SPEEDUP FACTOR:                                             1.62x   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Note: These are representative results. Actual speedup varies by:
  - CPU architecture (Intel processors get best optimization)
  - Model size and complexity
  - Input sequence length
  - Batch size

To run actual benchmarks:
  pip install optimum[openvino] transformers torch
  python examples/openvino_benchmark.py
""")
    
    # =================================================================
    # Summary
    # =================================================================
    divider("Summary")
    
    print("""
OpenVINO Integration Benefits:

âœ… Reduced Inference Latency
   - Typically 1.5x - 3x faster on Intel CPUs
   - Optimized for Intel hardware (CPU, iGPU, VPU)

âœ… Lower Memory Usage  
   - Model quantization (INT8, FP16)
   - Efficient memory management

âœ… Easy Integration
   - Drop-in replacement via optimum-intel
   - Same API as PyTorch models

âœ… Production Ready
   - Stable for deployment
   - Extensive model support

Framework Integration:
  - OpenVINOTextClassifier: Sentiment/classification tasks
  - OpenVINOEmbedding: Vector embeddings for RAG/search
  - Both integrate with Tool base class for workflows
""")
    
    print("âœ… Benchmark demo completed!")
